# Federated Learning Privacy Attacks (FLPA)

This project demonstrates privacy vulnerabilities in federated learning (FL) systems through systematic membership inference attacks (MIAs). It includes a complete modular framework for federated training, logging, attack evaluation, and visualization.

## Overview

The project implements:
- Federated learning simulation using [Flower](https://flower.dev/) with the CIFAR-10 dataset
- Support for multiple model architectures:
  - Shallow CNN (baseline)
  - Deep ResNet-18 model (achieving ~80% accuracy)
- Support for multiple FL strategies:
  - FedAvg
  - FedAdam
- Comprehensive membership inference attacks:
  - Posterior-based (black-box)
  - Gradient-based (white-box)
  - Activation-based (white-box)
  - Fusion attack (posterior + gradient)
- End-to-end experiment reproducibility with deterministic partitioning and ground-truth membership tracking
- Metrics logging and attack result visualization

## Prerequisites

- [uv](https://github.com/astral-sh/uv) â€” Fast Python package installer and virtual environment manager

## Installation

### 1ï¸âƒ£ Install `uv` (Linux)

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### 2ï¸âƒ£ Clone the repository

```bash
git clone git@github.com:hetparekh16/fl-privacy-attacks.git
cd fl-privacy-attacks
```

### 3ï¸âƒ£ Install dependencies

```bash
uv sync
```

## Usage

### Step 1: Prepare CIFAR-10 dataset

```bash
uv run src/flpa/dataset.py
```

This downloads CIFAR-10 to the `data/` directory and prepares deterministic client partitions.

### Step 2: Run federated training

```bash
uv run flwr run . local-simulation-gpu
```

This will:
- Train either a shallow CNN or ResNet-18 (controlled via config)
- Select optimizer strategy (FedAvg or FedAdam)
- Log training metrics and save the final global model at `outputs/global_model/`

### Step 3: Run membership inference attacks

```bash
uv run src/flpa/attacks/run_attack_pipeline.py
```

This pipeline executes:
1. Build membership dataset from logged sample IDs
2. Extract posteriors, gradients, and activations from final model
3. Train and evaluate attack models (Logistic Regression, Random Forest, MLP)

Attack results (metrics + models) are stored under `outputs/attacks/`.

### Step 4: Visualize results (optional)

```bash
uv run streamlit run apps/streamlit_app.py
```

Features:
- Per-client training curves
- Global validation accuracy and loss
- Attack effectiveness (ROC curves, confusion matrices, metrics)

## Project Structure

```
â”œâ”€â”€ src/flpa
â”‚   â”œâ”€â”€ client_app.py         # Flower client
â”‚   â”œâ”€â”€ server_app.py         # Flower server
â”‚   â”œâ”€â”€ dataset.py            # CIFAR-10 dataset preparation
â”‚   â”œâ”€â”€ task.py               # Model and training logic (CNN, ResNet-18)
â”‚   â”œâ”€â”€ fed_strategies/       # LoggingFedAvg, LoggingFedAdam
â”‚   â”œâ”€â”€ attacks/
â”‚   â”‚   â”œâ”€â”€ posterior/        # Posterior-based MIA
â”‚   â”‚   â”œâ”€â”€ gradient_based/   # Gradient-based MIA
â”‚   â”‚   â”œâ”€â”€ activation_based/ # Activation-based MIA
â”‚   â”‚   â”œâ”€â”€ fusion_posterior_and_gradient/ # Fusion attack
â”‚   â”‚   â””â”€â”€ base/             # Membership dataset construction
â”‚   â””â”€â”€ utils.py              # Utilities (logging, helpers)
â”œâ”€â”€ apps                      # Streamlit dashboard
â”œâ”€â”€ data                      # CIFAR-10 dataset storage
â”œâ”€â”€ outputs                   # All generated logs, models, metrics
â”œâ”€â”€ pyproject.toml            # Dependency specification
â”œâ”€â”€ uv.lock                   # Dependency lock file
â””â”€â”€ README.md
```

## Key Features

ğŸ“¦ Modular design for attack extensibility

ğŸ”’ Insider and outsider threat simulation (black-box and white-box attacks)

ğŸ“ˆ Robust metrics logging and visualization

ğŸ–¥ï¸ GPU acceleration ready

ğŸ” Fully reproducible experiments with deterministic partitioning and ground-truth membership tracking